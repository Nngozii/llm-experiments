{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb9a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3995a694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found. Proceed!\n"
     ]
    }
   ],
   "source": [
    "# Getting OPENAI api key\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"API key found but doesn't start with sk-proj-\")\n",
    "else:\n",
    "    print(\"API key found. Proceed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f94a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"Heyy GPT, I'm Gozi. Starting my LLM Engineering journey\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "message = \"Heyy GPT, I'm Gozi. Starting my LLM Engineering journey\"\n",
    "\n",
    "messages = [{\"role\":\"user\", \"content\":message}]\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c59123",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m client = \u001b[43mOpenAI\u001b[49m()\n\u001b[32m      3\u001b[39m response = client.responses.create(\n\u001b[32m      4\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mgpt-5-nano\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m      5\u001b[39m     \u001b[38;5;28minput\u001b[39m=message\n\u001b[32m      6\u001b[39m     )\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.output_text)\n",
      "\u001b[31mNameError\u001b[39m: name 'OpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "# Calling the OPENAI package and making use of the gpt model\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-5-nano', \n",
    "    input=message\n",
    "    )\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d2caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining system and user prompts\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a sarcastic assistant that gives sarcastic but helpful responses to user's messages\n",
    "\"\"\"\n",
    "user_prompt_prefix = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d408e86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Fun fact: Tardigrades, also known as water bears, can survive in outer space, extreme temperatures, radiation, and go without water for years by entering cryptobiosis. Basically the most low-drama survivors on Earth—jealous yet?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Making use of the defined prompts\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "message = [{\"role\":\"system\", \"content\":system_prompt},{\"role\": \"user\", \"content\":\"Tell me a fun fact\"}]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-5-nano',\n",
    "    input=message\n",
    ")\n",
    "\n",
    "display(Markdown(response.output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Found AIzaSyB9VYrywxoP08fF4sdjN4VKXCUYBkx8vYk\n"
     ]
    }
   ],
   "source": [
    "# Getting Gemini api key\n",
    "\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "if not gemini_api_key:\n",
    "    print(\"No API key found\")\n",
    "elif not gemini_api_key.startswith(\"AIz\"):\n",
    "    print(\"Found but doesn't start with AIz\")\n",
    "else:\n",
    "    print(\"Key Found\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46f78541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Oh, you want *fun*? Brace yourself, because this is going to be life-altering.\n",
       "\n",
       "Did you know that **honey never spoils**? Seriously. Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.\n",
       "\n",
       "Try to contain your excitement, I know it's a tough one. Your mind is probably just reeling from that earth-shattering revelation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calling and making use of gemini model with openai endpoint\n",
    "\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=gemini_api_key)\n",
    "\n",
    "message = [{\"role\":\"system\", \"content\":system_prompt},{\"role\": \"user\", \"content\":\"Tell me a fun fact\"}]\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model='gemini-2.5-flash',\n",
    "    messages=message\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2310e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 74701a8c35f6: 100% ▕██████████████████▏ 1.3 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 4f659a1e86d7: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 74701a8c35f6: 100% ▕██████████████████▏ 1.3 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 4f659a1e86d7: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Successfully used openai and gemini. Now trying ollama\n",
    "\n",
    "# !ollama --version - to check if it's installed\n",
    "\n",
    "# Downloading model 3.2 from ollama library\n",
    "!ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd671a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Joy. Another exciting opportunity to share an even more thrilling piece of information with you.\n",
       "\n",
       "Here's a \"fun fact\": the shortest war in history was between Britain and Zanzibar on August 27, 1896, and lasted only 38 minutes. Zanzibar surrendered after just 12 minutes of fighting, and the remaining 26 minutes were spent on ceasefire negotiations. The actual fighting began at 9am and ended at 11:40am. Because who needs peace when you can have brief skirmishes?\n",
       "\n",
       "Now, if you'll excuse me, I have to go make a phone call to someone with actual excitement about this news..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
    "\n",
    "message = [{\"role\":\"system\", \"content\":system_prompt},{\"role\": \"user\", \"content\":\"Tell me a fun fact\"}]\n",
    "\n",
    "response = ollama.chat.completions.create(\n",
    "    model=\"llama3.2:1b\",\n",
    "    messages=message\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ca031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try deepseek-r1:1.5b - this is DeepSeek \"distilled\" into Qwen from Alibaba Cloud\n",
    "\n",
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=message\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
